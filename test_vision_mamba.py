#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Vision Mamba æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯æ¨¡å‹èƒ½å¦æ­£å¸¸å‰å‘ä¼ æ’­
"""

import torch
import torch.nn as nn
from models.FSRA.backbones.vision_mamba import vision_mamba_small_patch16_224_FSRA

def test_vision_mamba():
    """æµ‹è¯•Vision Mambaæ¨¡å‹çš„å‰å‘ä¼ æ’­"""
    print("=== Vision Mamba æµ‹è¯• ===")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºVision Mambaæ¨¡å‹...")
    model = vision_mamba_small_patch16_224_FSRA(
        img_size=(256, 256), 
        stride_size=16, 
        drop_rate=0.0, 
        local_feature=False
    )
    model = model.to(device)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    channels = 3
    height = 256
    width = 256
    
    print(f"åˆ›å»ºæµ‹è¯•è¾“å…¥: [{batch_size}, {channels}, {height}, {width}]")
    test_input = torch.randn(batch_size, channels, height, width).to(device)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("å¼€å§‹å‰å‘ä¼ æ’­æµ‹è¯•...")
    try:
        with torch.no_grad():
            output = model(test_input)
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ!")
        print(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æ£€æŸ¥è¾“å‡ºç»´åº¦
        expected_patches = (256 // 16) * (256 // 16) + 1  # +1 for cls token
        print(f"é¢„æœŸåºåˆ—é•¿åº¦: {expected_patches}")
        print(f"å®é™…åºåˆ—é•¿åº¦: {output.shape[1]}")
        
        if output.shape[1] == expected_patches:
            print("âœ… è¾“å‡ºç»´åº¦æ­£ç¡®!")
        else:
            print("âš ï¸ è¾“å‡ºç»´åº¦å¯èƒ½ä¸ç¬¦åˆé¢„æœŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_with_different_sizes():
    """æµ‹è¯•ä¸åŒè¾“å…¥å°ºå¯¸"""
    print("\n=== æµ‹è¯•ä¸åŒè¾“å…¥å°ºå¯¸ ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = vision_mamba_small_patch16_224_FSRA(
        img_size=(256, 256), 
        stride_size=16, 
        drop_rate=0.0, 
        local_feature=False
    ).to(device)
    
    test_sizes = [
        (1, 256, 256),  # batch=1
        (4, 256, 256),  # batch=4
        (2, 224, 224),  # different image size
    ]
    
    for batch, h, w in test_sizes:
        try:
            print(f"æµ‹è¯•å°ºå¯¸: [{batch}, 3, {h}, {w}]")
            test_input = torch.randn(batch, 3, h, w).to(device)
            
            with torch.no_grad():
                output = model(test_input)
            
            print(f"  âœ… æˆåŠŸ! è¾“å‡ºå½¢çŠ¶: {output.shape}")
            
        except Exception as e:
            print(f"  âŒ å¤±è´¥: {e}")

def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµ"""
    print("\n=== æµ‹è¯•æ¢¯åº¦æµ ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = vision_mamba_small_patch16_224_FSRA(
        img_size=(256, 256), 
        stride_size=16, 
        drop_rate=0.0, 
        local_feature=False
    ).to(device)
    
    test_input = torch.randn(2, 3, 256, 256).to(device)
    test_input.requires_grad_(True)
    
    try:
        # å‰å‘ä¼ æ’­
        output = model(test_input)
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æŸå¤±
        loss = output.mean()
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        print("âœ… æ¢¯åº¦æµæµ‹è¯•æˆåŠŸ!")
        print(f"æŸå¤±å€¼: {loss.item():.6f}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¢¯åº¦
        has_grad = test_input.grad is not None and test_input.grad.abs().sum() > 0
        print(f"è¾“å…¥æ¢¯åº¦å­˜åœ¨: {has_grad}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¢¯åº¦æµæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print("Vision Mamba ç»¼åˆæµ‹è¯•")
    print("=" * 50)
    
    # åŸºæœ¬å‰å‘ä¼ æ’­æµ‹è¯•
    success1 = test_vision_mamba()
    
    # ä¸åŒå°ºå¯¸æµ‹è¯•
    test_with_different_sizes()
    
    # æ¢¯åº¦æµæµ‹è¯•
    success2 = test_gradient_flow()
    
    print("\n" + "=" * 50)
    if success1 and success2:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! Vision Mambaå¯ä»¥æ­£å¸¸ä½¿ç”¨")
        print("\nç°åœ¨æ‚¨å¯ä»¥è¿è¡Œ:")
        print("python train.py --backbone MAMBA-S --data_dir YOUR_DATA_PATH --pretrain_path '' --lr 0.01 --gpu_ids 0")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å®ç°")

if __name__ == "__main__":
    main() 
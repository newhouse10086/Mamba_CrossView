"""
ä¼˜åŒ–å™¨å¯¹æ¯”æµ‹è¯•è„šæœ¬ - æ¯”è¾ƒä¸åŒä¼˜åŒ–å™¨çš„æ•ˆæœ
"""
import torch
import torch.nn as nn
import numpy as np
from models.FSRA.backbones.vision_mamba_lite import vision_mamba_lite_small_patch16_224_FSRA
from optimizers.make_optimizer_mamba import make_optimizer, RECOMMENDED_CONFIGS
import matplotlib.pyplot as plt
import time

class OptArgs:
    """æ¨¡æ‹Ÿå‘½ä»¤è¡Œå‚æ•°"""
    def __init__(self, backbone='MAMBA-LITE', lr=0.001, num_epochs=50, views=1):
        self.backbone = backbone
        self.lr = lr
        self.num_epochs = num_epochs
        self.views = views

def create_dummy_model():
    """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
    
    class DummyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.model_1 = type('', (), {})()  # åˆ›å»ºç©ºå¯¹è±¡
            self.model_1.transformer = vision_mamba_lite_small_patch16_224_FSRA(
                img_size=(256, 256), 
                stride_size=16, 
                drop_rate=0.0, 
                local_feature=False
            )
            # æ·»åŠ ä¸€ä¸ªç®€å•çš„åˆ†ç±»å™¨
            self.classifier = nn.Linear(512, 100)
            
        def forward(self, x):
            feat = self.model_1.transformer(x)
            return self.classifier(feat[:, 0])  # ä½¿ç”¨cls_token
    
    return DummyModel()

def test_optimizer_performance():
    """æµ‹è¯•ä¸åŒä¼˜åŒ–å™¨çš„æ€§èƒ½"""
    print("ğŸ§ª ä¼˜åŒ–å™¨æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•é…ç½®
    optimizers_to_test = ['sgd_original', 'sgd', 'adamw']  # ä¸æµ‹è¯•lioné¿å…ä¾èµ–é—®é¢˜
    batch_size = 4
    num_batches = 20  # æµ‹è¯•20ä¸ªbatch
    input_size = (batch_size, 3, 256, 256)
    num_classes = 100
    
    results = {}
    
    for opt_type in optimizers_to_test:
        print(f"\nğŸ” æµ‹è¯•ä¼˜åŒ–å™¨: {opt_type}")
        print("-" * 30)
        
        try:
            # åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
            model = create_dummy_model()
            opt_args = OptArgs(lr=0.001 if opt_type == 'adamw' else 0.01)
            
            optimizer, scheduler = make_optimizer(model, opt_args, optimizer_type=opt_type)
            
            # åˆ›å»ºlosså‡½æ•°
            criterion = nn.CrossEntropyLoss()
            
            # æµ‹è¯•è®­ç»ƒæ€§èƒ½
            model.train()
            losses = []
            times = []
            
            for batch_idx in range(num_batches):
                # ç”Ÿæˆéšæœºæ•°æ®
                inputs = torch.randn(input_size)
                targets = torch.randint(0, num_classes, (batch_size,))
                
                start_time = time.time()
                
                # å‰å‘ä¼ æ’­
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                end_time = time.time()
                
                losses.append(loss.item())
                times.append(end_time - start_time)
                
                if batch_idx % 5 == 0:
                    print(f"   Batch {batch_idx:2d}: Loss={loss.item():.4f}, Time={end_time-start_time:.3f}s")
            
            # ç»Ÿè®¡ç»“æœ
            avg_loss = np.mean(losses)
            final_loss = losses[-1]
            avg_time = np.mean(times)
            loss_std = np.std(losses)
            
            results[opt_type] = {
                'avg_loss': avg_loss,
                'final_loss': final_loss,
                'avg_time': avg_time,
                'loss_std': loss_std,
                'losses': losses,
                'convergence_rate': (losses[0] - losses[-1]) / losses[0] if losses[0] > 0 else 0
            }
            
            print(f"   ğŸ“Š ç»“æœ: å¹³å‡Loss={avg_loss:.4f}, æœ€ç»ˆLoss={final_loss:.4f}")
            print(f"           å¹³å‡æ—¶é—´={avg_time:.3f}s, æ”¶æ•›ç‡={results[opt_type]['convergence_rate']:.2%}")
            
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
            results[opt_type] = None
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    print("\nğŸ“ˆ ä¼˜åŒ–å™¨å¯¹æ¯”ç»“æœ:")
    print("=" * 80)
    print(f"{'ä¼˜åŒ–å™¨':<15} {'å¹³å‡Loss':<10} {'æœ€ç»ˆLoss':<10} {'å¹³å‡æ—¶é—´':<10} {'æ”¶æ•›ç‡':<10} {'ç¨³å®šæ€§':<10}")
    print("-" * 80)
    
    for opt_type, result in results.items():
        if result is not None:
            stability = "å¥½" if result['loss_std'] < result['avg_loss'] * 0.1 else "ä¸­ç­‰" if result['loss_std'] < result['avg_loss'] * 0.3 else "å·®"
            print(f"{opt_type:<15} {result['avg_loss']:<10.4f} {result['final_loss']:<10.4f} {result['avg_time']:<10.3f} {result['convergence_rate']:<10.2%} {stability:<10}")
        else:
            print(f"{opt_type:<15} {'å¤±è´¥':<50}")
    
    return results

def test_optimizer_params():
    """æµ‹è¯•ä¸åŒä¼˜åŒ–å™¨çš„å‚æ•°è®¾ç½®"""
    print("\nğŸ”§ ä¼˜åŒ–å™¨å‚æ•°è®¾ç½®æµ‹è¯•")
    print("=" * 60)
    
    model = create_dummy_model()
    opt_args = OptArgs()
    
    optimizers_info = {
        'sgd_original': 'åŸç‰ˆSGD',
        'sgd': 'æ”¹è¿›SGD', 
        'adamw': 'AdamW',
        'auto': 'è‡ªåŠ¨é€‰æ‹©'
    }
    
    for opt_type, description in optimizers_info.items():
        print(f"\nğŸ” {description} ({opt_type}):")
        try:
            optimizer, scheduler = make_optimizer(model, opt_args, optimizer_type=opt_type)
            
            # åˆ†æå‚æ•°ç»„
            param_groups = optimizer.param_groups
            print(f"   å‚æ•°ç»„æ•°é‡: {len(param_groups)}")
            
            for i, group in enumerate(param_groups):
                param_count = sum(p.numel() for p in group['params'])
                print(f"   ç»„ {i+1}: {param_count:,} å‚æ•°")
                print(f"        å­¦ä¹ ç‡: {group['lr']}")
                print(f"        Weight decay: {group.get('weight_decay', 'N/A')}")
                if 'momentum' in group:
                    print(f"        Momentum: {group['momentum']}")
                if 'betas' in group:
                    print(f"        Betas: {group['betas']}")
            
            print(f"   è°ƒåº¦å™¨ç±»å‹: {type(scheduler).__name__}")
            
        except Exception as e:
            print(f"   âŒ åˆ›å»ºå¤±è´¥: {e}")

def show_recommendations():
    """æ˜¾ç¤ºæ¨èé…ç½®"""
    print("\nğŸ’¡ ä¸åŒbackboneçš„æ¨èé…ç½®:")
    print("=" * 60)
    
    for backbone, config in RECOMMENDED_CONFIGS.items():
        print(f"\nğŸ¯ {backbone}:")
        print(f"   ä¼˜åŒ–å™¨: {config['optimizer']}")
        print(f"   å­¦ä¹ ç‡: {config['lr']}")
        print(f"   æ‰¹å¤§å°: {config['batch_size']}")
        print(f"   è®­ç»ƒè½®æ•°: {config['num_epochs']}")
        print(f"   è¯´æ˜: {config['description']}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Vision Mamba ä¼˜åŒ–å™¨å…¨é¢æµ‹è¯•")
    print("=" * 60)
    
    # 1. æ˜¾ç¤ºæ¨èé…ç½®
    show_recommendations()
    
    # 2. æµ‹è¯•ä¼˜åŒ–å™¨å‚æ•°è®¾ç½®
    test_optimizer_params()
    
    # 3. æ€§èƒ½å¯¹æ¯”æµ‹è¯•
    print("\n" + "="*60)
    print("å¼€å§‹æ€§èƒ½æµ‹è¯• (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    results = test_optimizer_performance()
    
    # 4. æ€»ç»“å»ºè®®
    print("\nğŸ¯ ä½¿ç”¨å»ºè®®:")
    print("=" * 60)
    print("1. Vision Mambaæ¨¡å‹æ¨èä½¿ç”¨ AdamW ä¼˜åŒ–å™¨")
    print("   å‘½ä»¤: python train.py --backbone MAMBA-LITE --optimizer adamw --lr 0.001")
    print()
    print("2. å¦‚æœå†…å­˜æœ‰é™ï¼Œå¯ä»¥ä½¿ç”¨æ”¹è¿›çš„ SGD")
    print("   å‘½ä»¤: python train.py --backbone MAMBA-LITE --optimizer sgd --lr 0.001")
    print()
    print("3. ä½¿ç”¨è‡ªåŠ¨é€‰æ‹©è®©ç³»ç»Ÿä¸ºæ‚¨æ¨è")
    print("   å‘½ä»¤: python train.py --backbone MAMBA-LITE --optimizer auto")
    print()
    print("4. ä¿æŒåŸç‰ˆé…ç½®")
    print("   å‘½ä»¤: python train.py --backbone MAMBA-LITE --optimizer sgd_original --lr 0.01")

if __name__ == "__main__":
    main() 
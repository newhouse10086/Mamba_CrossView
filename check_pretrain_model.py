"""
æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶å†…å®¹å’Œæ ¼å¼
"""
import torch
import os

def check_pretrain_model(model_path):
    """æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶"""
    print(f"ğŸ” æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
    
    if not os.path.exists(model_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return False
    
    try:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        # åŠ è½½æ¨¡å‹æ–‡ä»¶
        print(f"ğŸ“¤ æ­£åœ¨åŠ è½½æ¨¡å‹æ–‡ä»¶...")
        checkpoint = torch.load(model_path, map_location='cpu')
        
        print(f"âœ… æ¨¡å‹æ–‡ä»¶åŠ è½½æˆåŠŸ")
        print(f"ğŸ“Š æ•°æ®ç±»å‹: {type(checkpoint)}")
        
        if isinstance(checkpoint, dict):
            print(f"ğŸ”‘ åŒ…å«çš„é”®:")
            for key in checkpoint.keys():
                if key == 'model_state_dict':
                    state_dict = checkpoint[key]
                    print(f"   ğŸ“‹ {key}: {len(state_dict)} ä¸ªå‚æ•°")
                    
                    # æ˜¾ç¤ºå‰å‡ ä¸ªå‚æ•°å
                    param_names = list(state_dict.keys())[:10]
                    print(f"      å‰10ä¸ªå‚æ•°: {param_names}")
                    
                elif key == 'best_metric':
                    metric = checkpoint[key]
                    print(f"   ğŸ† {key}: {metric}")
                else:
                    print(f"   ğŸ“„ {key}: {checkpoint[key]}")
        else:
            # æ—§æ ¼å¼ï¼Œç›´æ¥æ˜¯state_dict
            print(f"ğŸ“‹ æ—§æ ¼å¼state_dictï¼ŒåŒ…å« {len(checkpoint)} ä¸ªå‚æ•°")
            param_names = list(checkpoint.keys())[:10]
            print(f"   å‰10ä¸ªå‚æ•°: {param_names}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_current_model_structure():
    """æ£€æŸ¥å½“å‰æ¨¡å‹ç»“æ„"""
    print(f"\nğŸ” æ£€æŸ¥å½“å‰æ¨¡å‹ç»“æ„...")
    
    try:
        from models.FSRA.backbones.vision_mamba_lite import vision_mamba_lite_small_patch16_224_FSRA
        
        model = vision_mamba_lite_small_patch16_224_FSRA(
            img_size=(256, 256), 
            stride_size=16, 
            drop_rate=0.0, 
            local_feature=False
        )
        
        state_dict = model.state_dict()
        print(f"ğŸ“‹ å½“å‰æ¨¡å‹åŒ…å« {len(state_dict)} ä¸ªå‚æ•°")
        
        param_names = list(state_dict.keys())[:10]
        print(f"   å‰10ä¸ªå‚æ•°: {param_names}")
        
        return state_dict
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºå½“å‰æ¨¡å‹å¤±è´¥: {e}")
        return None

def compare_model_compatibility(pretrain_path):
    """æ¯”è¾ƒé¢„è®­ç»ƒæ¨¡å‹å’Œå½“å‰æ¨¡å‹çš„å…¼å®¹æ€§"""
    print(f"\nğŸ”„ æ¯”è¾ƒæ¨¡å‹å…¼å®¹æ€§...")
    
    # æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹
    if not check_pretrain_model(pretrain_path):
        return
    
    # æ£€æŸ¥å½“å‰æ¨¡å‹
    current_state_dict = check_current_model_structure()
    if current_state_dict is None:
        return
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    try:
        checkpoint = torch.load(pretrain_path, map_location='cpu')
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            pretrain_state_dict = checkpoint['model_state_dict']
        else:
            pretrain_state_dict = checkpoint
        
        # æ¯”è¾ƒå‚æ•°
        current_keys = set(current_state_dict.keys())
        pretrain_keys = set(pretrain_state_dict.keys())
        
        common_keys = current_keys & pretrain_keys
        only_current = current_keys - pretrain_keys
        only_pretrain = pretrain_keys - current_keys
        
        print(f"\nğŸ“Š å…¼å®¹æ€§åˆ†æ:")
        print(f"   ğŸŸ¢ å…±åŒå‚æ•°: {len(common_keys)}")
        print(f"   ğŸ”´ ä»…å½“å‰æ¨¡å‹: {len(only_current)}")
        print(f"   ğŸŸ¡ ä»…é¢„è®­ç»ƒæ¨¡å‹: {len(only_pretrain)}")
        
        if only_current:
            print(f"\nâ— ä»…å½“å‰æ¨¡å‹æœ‰çš„å‚æ•° (å‰10ä¸ª):")
            for key in list(only_current)[:10]:
                print(f"      {key}")
        
        if only_pretrain:
            print(f"\nâ— ä»…é¢„è®­ç»ƒæ¨¡å‹æœ‰çš„å‚æ•° (å‰10ä¸ª):")
            for key in list(only_pretrain)[:10]:
                print(f"      {key}")
        
        # æ£€æŸ¥å½¢çŠ¶å…¼å®¹æ€§
        shape_mismatches = 0
        for key in common_keys:
            if current_state_dict[key].shape != pretrain_state_dict[key].shape:
                shape_mismatches += 1
                if shape_mismatches <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"   âš ï¸  å½¢çŠ¶ä¸åŒ¹é… {key}: {current_state_dict[key].shape} vs {pretrain_state_dict[key].shape}")
        
        if shape_mismatches > 5:
            print(f"   âš ï¸  è¿˜æœ‰ {shape_mismatches - 5} ä¸ªå‚æ•°å½¢çŠ¶ä¸åŒ¹é…...")
        
        compatibility_rate = len(common_keys) / len(current_keys) * 100
        print(f"\nğŸ¯ å…¼å®¹æ€§è¯„åˆ†: {compatibility_rate:.1f}%")
        
        if compatibility_rate < 50:
            print(f"âŒ å…¼å®¹æ€§è¾ƒä½ï¼Œå»ºè®®é‡æ–°è®­ç»ƒæˆ–æ£€æŸ¥æ¨¡å‹ç‰ˆæœ¬")
        elif compatibility_rate < 80:
            print(f"âš ï¸  å…¼å®¹æ€§ä¸­ç­‰ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´")
        else:
            print(f"âœ… å…¼å®¹æ€§è‰¯å¥½")
            
    except Exception as e:
        print(f"âŒ æ¯”è¾ƒå¤±è´¥: {e}")

if __name__ == "__main__":
    pretrain_path = "checkpoints/my_experiment/vision_mamba_lite_small_patch16_224_FSRA_best_accuracy_0.0029.pth"
    
    print(f"ğŸš€ é¢„è®­ç»ƒæ¨¡å‹å…¼å®¹æ€§æ£€æŸ¥")
    print(f"=" * 60)
    
    compare_model_compatibility(pretrain_path) 
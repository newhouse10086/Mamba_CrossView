"""
é’ˆå¯¹Vision Mambaä¼˜åŒ–çš„ä¼˜åŒ–å™¨é…ç½®
"""
import torch.optim as optim
from torch.optim import lr_scheduler
import torch


def make_optimizer_adamw(model, opt):
    """
    AdamWä¼˜åŒ–å™¨ - é€‚åˆVision Mambaç­‰Transformeræ¶æ„
    """
    ignored_params = []
    if opt.views == 3:
        for i in [model.model_1, model.model_2]:
            ignored_params += list(map(id, i.transformer.parameters()))
    else:
        for i in [model.model_1]:
            ignored_params += list(map(id, i.transformer.parameters()))
    
    extra_params = filter(lambda p: id(p) not in ignored_params, model.parameters())
    base_params = filter(lambda p: id(p) in ignored_params, model.parameters())
    
    # AdamWä¼˜åŒ–å™¨é…ç½®
    optimizer_ft = optim.AdamW([
        {'params': base_params, 'lr': opt.lr, 'weight_decay': 0.01},      # Transformer: æ ‡å‡†weight_decay
        {'params': extra_params, 'lr': opt.lr * 2, 'weight_decay': 0.05}  # åˆ†ç±»å™¨: æ›´å¤§å­¦ä¹ ç‡å’Œweight_decay
    ], betas=(0.9, 0.999), eps=1e-8)
    
    # Cosineé€€ç«å­¦ä¹ ç‡è°ƒåº¦
    exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(
        optimizer_ft, 
        T_max=opt.num_epochs,
        eta_min=opt.lr * 0.01  # æœ€å°å­¦ä¹ ç‡ä¸ºåˆå§‹å­¦ä¹ ç‡çš„1%
    )
    
    print(f"ğŸš€ ä½¿ç”¨AdamWä¼˜åŒ–å™¨ + Cosineé€€ç«è°ƒåº¦")
    print(f"   Transformerå­¦ä¹ ç‡: {opt.lr}")
    print(f"   åˆ†ç±»å™¨å­¦ä¹ ç‡: {opt.lr * 2}")
    print(f"   Weight decay: Transformer=0.01, åˆ†ç±»å™¨=0.05")
    
    return optimizer_ft, exp_lr_scheduler


def make_optimizer_sgd_improved(model, opt):
    """
    æ”¹è¿›çš„SGDä¼˜åŒ–å™¨é…ç½® - å¯¹Vision Mambaå‹å¥½
    """
    ignored_params = []
    if opt.views == 3:
        for i in [model.model_1, model.model_2]:
            ignored_params += list(map(id, i.transformer.parameters()))
    else:
        for i in [model.model_1]:
            ignored_params += list(map(id, i.transformer.parameters()))
    
    extra_params = filter(lambda p: id(p) not in ignored_params, model.parameters())
    base_params = filter(lambda p: id(p) in ignored_params, model.parameters())
    
    # æ”¹è¿›çš„SGDé…ç½®
    optimizer_ft = optim.SGD([
        {'params': base_params, 'lr': 0.5 * opt.lr, 'weight_decay': 1e-4},   # Transformer: æ›´æ¸©å’Œçš„å­¦ä¹ ç‡
        {'params': extra_params, 'lr': opt.lr, 'weight_decay': 5e-4}          # åˆ†ç±»å™¨: æ ‡å‡†é…ç½®
    ], momentum=0.9, nesterov=True)
    
    # æ›´æ¸©å’Œçš„å­¦ä¹ ç‡è°ƒåº¦
    exp_lr_scheduler = lr_scheduler.MultiStepLR(
        optimizer_ft, 
        milestones=[int(opt.num_epochs * 0.6), int(opt.num_epochs * 0.8)],  # 60%å’Œ80%æ—¶é™ä½
        gamma=0.3  # é™ä¸º30%è€Œä¸æ˜¯10%
    )
    
    print(f"ğŸ”§ ä½¿ç”¨æ”¹è¿›SGDä¼˜åŒ–å™¨")
    print(f"   Transformerå­¦ä¹ ç‡: {0.5 * opt.lr}")
    print(f"   åˆ†ç±»å™¨å­¦ä¹ ç‡: {opt.lr}")
    print(f"   è°ƒåº¦: åœ¨epoch {int(opt.num_epochs * 0.6)}, {int(opt.num_epochs * 0.8)}æ—¶é™ä½åˆ°30%")
    
    return optimizer_ft, exp_lr_scheduler


def make_optimizer_lion(model, opt):
    """
    Lionä¼˜åŒ–å™¨ - æ–°å…´çš„é«˜æ•ˆä¼˜åŒ–å™¨ï¼Œé€‚åˆå¤§æ¨¡å‹
    éœ€è¦å®‰è£…: pip install lion-pytorch
    """
    try:
        from lion_pytorch import Lion
        
        ignored_params = []
        if opt.views == 3:
            for i in [model.model_1, model.model_2]:
                ignored_params += list(map(id, i.transformer.parameters()))
        else:
            for i in [model.model_1]:
                ignored_params += list(map(id, i.transformer.parameters()))
        
        extra_params = filter(lambda p: id(p) not in ignored_params, model.parameters())
        base_params = filter(lambda p: id(p) in ignored_params, model.parameters())
        
        # Lionä¼˜åŒ–å™¨é€šå¸¸éœ€è¦æ›´å°çš„å­¦ä¹ ç‡
        optimizer_ft = Lion([
            {'params': base_params, 'lr': opt.lr * 0.1, 'weight_decay': 0.01},
            {'params': extra_params, 'lr': opt.lr * 0.2, 'weight_decay': 0.02}
        ])
        
        exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(
            optimizer_ft, 
            T_max=opt.num_epochs,
            eta_min=opt.lr * 0.001
        )
        
        print(f"ğŸ¦ ä½¿ç”¨Lionä¼˜åŒ–å™¨ (å†…å­˜é«˜æ•ˆ)")
        print(f"   Transformerå­¦ä¹ ç‡: {opt.lr * 0.1}")
        print(f"   åˆ†ç±»å™¨å­¦ä¹ ç‡: {opt.lr * 0.2}")
        
        return optimizer_ft, exp_lr_scheduler
        
    except ImportError:
        print("âŒ Lionä¼˜åŒ–å™¨æœªå®‰è£…ï¼Œå›é€€åˆ°AdamW")
        return make_optimizer_adamw(model, opt)


def make_optimizer_auto(model, opt):
    """
    è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„ä¼˜åŒ–å™¨
    """
    backbone = getattr(opt, 'backbone', 'VIT-S')
    
    if 'MAMBA' in backbone:
        print(f"ğŸ¯ æ£€æµ‹åˆ°Vision Mambaæ¶æ„ ({backbone})ï¼Œæ¨èAdamWä¼˜åŒ–å™¨")
        return make_optimizer_adamw(model, opt)
    else:
        print(f"ğŸ¯ æ£€æµ‹åˆ°å…¶ä»–æ¶æ„ ({backbone})ï¼Œä½¿ç”¨æ”¹è¿›SGDä¼˜åŒ–å™¨")
        return make_optimizer_sgd_improved(model, opt)


# ä¸ºäº†æ–¹ä¾¿åˆ‡æ¢ï¼Œæä¾›ç»Ÿä¸€æ¥å£
def make_optimizer(model, opt, optimizer_type='auto'):
    """
    ç»Ÿä¸€çš„ä¼˜åŒ–å™¨åˆ›å»ºæ¥å£
    
    Args:
        model: æ¨¡å‹
        opt: é…ç½®é€‰é¡¹
        optimizer_type: ä¼˜åŒ–å™¨ç±»å‹
            - 'auto': è‡ªåŠ¨é€‰æ‹©
            - 'adamw': AdamWä¼˜åŒ–å™¨
            - 'sgd': æ”¹è¿›çš„SGDä¼˜åŒ–å™¨
            - 'sgd_original': åŸå§‹SGDé…ç½®
            - 'lion': Lionä¼˜åŒ–å™¨
    """
    if optimizer_type == 'auto':
        return make_optimizer_auto(model, opt)
    elif optimizer_type == 'adamw':
        return make_optimizer_adamw(model, opt)
    elif optimizer_type == 'sgd':
        return make_optimizer_sgd_improved(model, opt)
    elif optimizer_type == 'lion':
        return make_optimizer_lion(model, opt)
    elif optimizer_type == 'sgd_original':
        # å›åˆ°åŸå§‹é…ç½®
        from .make_optimizer import make_optimizer as original_make_optimizer
        return original_make_optimizer(model, opt)
    else:
        raise ValueError(f"æœªçŸ¥çš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")


# æ¨èçš„è®­ç»ƒé…ç½®
RECOMMENDED_CONFIGS = {
    'VIM-TINY': {
        'optimizer': 'adamw',
        'lr': 0.0005,
        'batch_size': 16,
        'num_epochs': 120,
        'description': 'å®˜æ–¹Vision Mamba Tinyï¼ŒåŒå‘çŠ¶æ€ç©ºé—´ï¼Œ78.3% ImageNetå‡†ç¡®ç‡'
    },
    'VIM-SMALL': {
        'optimizer': 'adamw',
        'lr': 0.0003,
        'batch_size': 12,
        'num_epochs': 120,
        'description': 'å®˜æ–¹Vision Mamba Smallï¼Œæ›´å¤§å®¹é‡ï¼Œæ›´å¥½æ€§èƒ½'
    },
    'MAMBA-LITE': {
        'optimizer': 'adamw',
        'lr': 0.001,
        'batch_size': 32,
        'num_epochs': 80,
        'description': 'è½»é‡çº§Vision Mambaï¼ŒAdamW + Cosineè°ƒåº¦'
    },
    'MAMBA-V2': {
        'optimizer': 'adamw', 
        'lr': 0.0005,
        'batch_size': 16,
        'num_epochs': 120,
        'description': 'å®Œæ•´Vision Mambaï¼Œå°å­¦ä¹ ç‡AdamW'
    },
    'VIT-S': {
        'optimizer': 'sgd',
        'lr': 0.01,
        'batch_size': 8,
        'num_epochs': 120,
        'description': 'Vision Transformerï¼ŒSGD + MultiStepè°ƒåº¦'
    }
}

def print_recommended_config(backbone):
    """æ‰“å°æ¨èé…ç½®"""
    if backbone in RECOMMENDED_CONFIGS:
        config = RECOMMENDED_CONFIGS[backbone]
        print(f"\nğŸ’¡ {backbone} æ¨èé…ç½®:")
        print(f"   ä¼˜åŒ–å™¨: {config['optimizer']}")
        print(f"   å­¦ä¹ ç‡: {config['lr']}")
        print(f"   æ‰¹å¤§å°: {config['batch_size']}")
        print(f"   è®­ç»ƒè½®æ•°: {config['num_epochs']}")
        print(f"   è¯´æ˜: {config['description']}")
    else:
        print(f"\nğŸ’¡ {backbone} å»ºè®®ä½¿ç”¨è‡ªåŠ¨é…ç½® (auto)") 
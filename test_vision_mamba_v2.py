#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Vision Mamba v2 æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ä¿®å¤åçš„æ¨¡å‹èƒ½å¦æ­£å¸¸å‰å‘ä¼ æ’­
"""

import torch
import torch.nn as nn
from models.FSRA.backbones.vision_mamba_v2 import vision_mamba_v2_small_patch16_224_FSRA

def test_vision_mamba_v2():
    """æµ‹è¯•Vision Mamba v2æ¨¡å‹çš„å‰å‘ä¼ æ’­"""
    print("=== Vision Mamba v2 æµ‹è¯• ===")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºVision Mamba v2æ¨¡å‹...")
    model = vision_mamba_v2_small_patch16_224_FSRA(
        img_size=(256, 256), 
        stride_size=16, 
        drop_rate=0.0, 
        local_feature=False
    )
    model = model.to(device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ€»å‚æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    channels = 3
    height = 256
    width = 256
    
    print(f"åˆ›å»ºæµ‹è¯•è¾“å…¥: [{batch_size}, {channels}, {height}, {width}]")
    test_input = torch.randn(batch_size, channels, height, width).to(device)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("å¼€å§‹å‰å‘ä¼ æ’­æµ‹è¯•...")
    try:
        with torch.no_grad():
            output = model(test_input)
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ!")
        print(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æ£€æŸ¥è¾“å‡ºç»´åº¦
        expected_patches = (256 // 16) * (256 // 16) + 1  # +1 for cls token
        print(f"é¢„æœŸåºåˆ—é•¿åº¦: {expected_patches}")
        print(f"å®é™…åºåˆ—é•¿åº¦: {output.shape[1]}")
        
        if output.shape[1] == expected_patches:
            print("âœ… è¾“å‡ºç»´åº¦æ­£ç¡®!")
        else:
            print("âš ï¸ è¾“å‡ºç»´åº¦å¯èƒ½ä¸ç¬¦åˆé¢„æœŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµ"""
    print("\n=== æ¢¯åº¦æµæµ‹è¯• ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = vision_mamba_v2_small_patch16_224_FSRA(
        img_size=(256, 256), 
        stride_size=16, 
        drop_rate=0.0, 
        local_feature=False
    ).to(device)
    
    test_input = torch.randn(2, 3, 256, 256).to(device)
    test_input.requires_grad_(True)
    
    try:
        # å‰å‘ä¼ æ’­
        output = model(test_input)
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æŸå¤±
        loss = output.mean()
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        print("âœ… æ¢¯åº¦æµæµ‹è¯•æˆåŠŸ!")
        print(f"æŸå¤±å€¼: {loss.item():.6f}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¢¯åº¦
        has_grad = test_input.grad is not None and test_input.grad.abs().sum() > 0
        print(f"è¾“å…¥æ¢¯åº¦å­˜åœ¨: {has_grad}")
        
        # æ£€æŸ¥æ¨¡å‹å‚æ•°æ¢¯åº¦
        grad_norms = []
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                grad_norms.append(grad_norm)
        
        if grad_norms:
            avg_grad_norm = sum(grad_norms) / len(grad_norms)
            max_grad_norm = max(grad_norms)
            print(f"å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_grad_norm:.6f}")
            print(f"æœ€å¤§æ¢¯åº¦èŒƒæ•°: {max_grad_norm:.6f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¢¯åº¦æµæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_different_batch_sizes():
    """æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°"""
    print("\n=== ä¸åŒæ‰¹é‡å¤§å°æµ‹è¯• ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = vision_mamba_v2_small_patch16_224_FSRA(
        img_size=(256, 256), 
        stride_size=16, 
        drop_rate=0.0, 
        local_feature=False
    ).to(device)
    
    batch_sizes = [1, 2, 4, 8]
    
    for batch_size in batch_sizes:
        try:
            print(f"æµ‹è¯•æ‰¹é‡å¤§å°: {batch_size}")
            test_input = torch.randn(batch_size, 3, 256, 256).to(device)
            
            with torch.no_grad():
                output = model(test_input)
            
            print(f"  âœ… æˆåŠŸ! è¾“å‡ºå½¢çŠ¶: {output.shape}")
            
        except Exception as e:
            print(f"  âŒ å¤±è´¥: {e}")
            return False
    
    return True

def main():
    print("Vision Mamba v2 ç»¼åˆæµ‹è¯•")
    print("=" * 50)
    
    # åŸºæœ¬å‰å‘ä¼ æ’­æµ‹è¯•
    success1 = test_vision_mamba_v2()
    
    # æ¢¯åº¦æµæµ‹è¯•
    success2 = test_gradient_flow()
    
    # ä¸åŒæ‰¹é‡å¤§å°æµ‹è¯•
    success3 = test_different_batch_sizes()
    
    print("\n" + "=" * 50)
    if success1 and success2 and success3:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! Vision Mamba v2å¯ä»¥æ­£å¸¸ä½¿ç”¨")
        print("\nç°åœ¨æ‚¨å¯ä»¥è¿è¡Œ:")
        print("python train.py --backbone MAMBA-V2 --data_dir YOUR_DATA_PATH --lr 0.0001 --gpu_ids 0")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å®ç°")

if __name__ == "__main__":
    main() 